{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and imports! <div class=\"tocSkip\">\n",
    "    \n",
    "`%run settings` executes `settings.py`. It contains most settings and imports.\n",
    "    \n",
    "About `autoreload` refer to [this page](https://ipython.org/ipython-doc/stable/config/extensions/autoreload.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%run settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out every value instead of just \"last_expr\" (default)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.getLogger().info(\"Logging INFOS.\")\n",
    "logging.getLogger().warning(\"Logging WARNINGS.\")\n",
    "logging.getLogger().error(\"Logging ERRORS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision for similarity values\n",
    "%precision 3\n",
    "np.set_printoptions(suppress=True) # no scientific for small numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining imports\n",
    "import pickle\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Hate Speech Detection and the Problem of Offensive Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../resources/auto_labled_data.csv\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(x):\n",
    "    if x == 0:\n",
    "        return \"hate_speech\"\n",
    "    if x == 1:\n",
    "        return \"offensive_language\"\n",
    "    if x == 2:\n",
    "        return \"neither\"\n",
    "\n",
    "\n",
    "df['class'] = df['class'].apply(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'tweet'\n",
    "\n",
    "label = 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of a classificator for Sentiment-Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                             min_df=10, \n",
    "                             max_df=0.3, \n",
    "                             lowercase=True,\n",
    "                             stop_words=None)\n",
    "\n",
    "X_tfidf = tfidf_vect.fit_transform(df[text_col])\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativ: X = X_tf\n",
    "X = X_tfidf\n",
    "y = df[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define holdout\n",
    "test_size = 0.2\n",
    "\n",
    "if test_size > 0.0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size,\n",
    "                                                        stratify = y,\n",
    "                                                        random_state=43\n",
    "                                                       )\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = X, None, y, None\n",
    "    \n",
    "    \n",
    "print(\"Trainig matrix:\", X_train.shape)\n",
    "print(\"Test matrix:    \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train_test'] = pd.Series(df.index.isin(y_test.index)).map(lambda x: 'Test' if x else 'Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train_test'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training on column {label}')\n",
    "\n",
    "clf = LinearSVC(C=1.0, max_iter=10000)\n",
    "\n",
    "clf.fit(X_train, y_train);\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print(f\"Classifier: {clf.__class__}\\n\")\n",
    "\n",
    "print('Accuracy Summary')\n",
    "print('================')\n",
    "\n",
    "print(f'Test:    {accuracy_score(y_test, y_test_pred)*100:6.2f}%')\n",
    "print(f'Train:   {accuracy_score(y_train, y_train_pred)*100:6.2f}%')\n",
    "print(f'Overall: {accuracy_score(y, y_pred)*100:6.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report\")\n",
    "print(\"=====================\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label names - specifies order in confusion matrix\n",
    "label_names = sorted(y_test.unique())\n",
    "\n",
    "# scale figure size depending on number of categories\n",
    "fsize = len(label_names)\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred, labels=label_names)\n",
    "\n",
    "_ = fig, ax = plt.subplots(figsize=(fsize, fsize))\n",
    "_ = sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "_ = plt.ylabel(\"Actual\")\n",
    "_ = plt.xlabel(\"Predicted\")\n",
    "_ = ax.set_title(f\"Confusion Matrix for {label}\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump model into a file and use it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(clf, open('senti_svc_model.pkl', 'wb'))\n",
    " \n",
    "# load the model from disk\n",
    "clf = pickle.load(open('senti_svc_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same goes for vectorizer (Transformation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "pickle.dump(tfidf_vect, open('tfidf_vect.pkl', 'wb'))\n",
    " \n",
    "# load the model from disk\n",
    "tfidf_vect = pickle.load(open('tfidf_vect.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test this model with a modified fox_news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_testset = pd.read_csv(\"../resources/fox_news.csv\", sep=\";\")\n",
    "df_testset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_foxnews(x):\n",
    "    if x == 0:\n",
    "        return \"neither\"\n",
    "    if x == 1:\n",
    "        return \"offensive_language\"\n",
    "\n",
    "\n",
    "df_testset['class'] = df_testset['class'].apply(classification_foxnews)\n",
    "df_testset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_foxnews = tfidf_vect.transform(df_testset['tweet'])\n",
    "y_foxnews = df_testset['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_foxnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Summary')\n",
    "print('================')\n",
    "\n",
    "print(f'Test:    {accuracy_score(y_foxnews, y_pred)*100:6.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_foxnews, y_pred, labels=label_names)\n",
    "\n",
    "_ = fig, ax = plt.subplots(figsize=(fsize, fsize))\n",
    "_ = sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "_ = plt.ylabel(\"Actual\")\n",
    "_ = plt.xlabel(\"Predicted\")\n",
    "_ = ax.set_title(f\"Confusion Matrix for {label}\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test an already existing model with some new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and vectorizer. (In order to get this model, the code has to be changed manually to use the fox_news dataset --> this should be changed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = pickle.load(open('../sentimentDetector/fox_news_model.pkl', 'rb'))\n",
    "tfidf_vect = pickle.load(open('../sentimentDetector/fox_news_vectorizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and prepare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../resources/auto_labled_data.csv\")\n",
    "df['class'] = df['class'].apply(classification)\n",
    "df = df.drop(columns=['Unnamed: 0', 'count', 'hate_speech', 'offensive_language', 'neither'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidf_vect.transform(df['tweet'])\n",
    "y = df['class']\n",
    "y_pred = clf.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy Summary')\n",
    "print('================')\n",
    "\n",
    "print(f'Test:    {accuracy_score(y, y_pred)*100:6.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y, y_pred, labels=label_names)\n",
    "\n",
    "_ = fig, ax = plt.subplots(figsize=(fsize, fsize))\n",
    "_ = sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, \n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "_ = plt.ylabel(\"Actual\")\n",
    "_ = plt.xlabel(\"Predicted\")\n",
    "_ = ax.set_title(f\"Confusion Matrix for {label}\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
